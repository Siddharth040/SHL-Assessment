{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c64aea",
   "metadata": {},
   "source": [
    "\n",
    "#  Grammar Scoring Model Report\n",
    "\n",
    "##  Approach\n",
    "\n",
    "We built a **Grammar Scoring Engine** that evaluates grammar usage in spoken English audio samples using deep learning techniques. The model scores inputs based on a predefined **rubric (1–5)** that evaluates grammar control, sentence structure, and complexity of language used.\n",
    "\n",
    "##  Preprocessing Steps\n",
    "\n",
    "1. **Audio Loading**: Audio files are loaded at 16kHz sampling rate.\n",
    "2. **Feature Extraction**: Using `Wav2Vec2` from HuggingFace Transformers to convert speech into embeddings.\n",
    "3. **Padding**: Ensures consistent input size across samples.\n",
    "4. **Label Mapping**: Targets were labeled according to the 1–5 grammar score rubric.\n",
    "\n",
    "##  Model Architecture\n",
    "\n",
    "- **Feature Extractor**: `Wav2Vec2` (`facebook/wav2vec2-base-960h`), pre-trained on large speech corpora.\n",
    "- **Classifier**: A simple fully connected feed-forward network on top of averaged Wav2Vec2 embeddings.\n",
    "\n",
    "Architecture summary:\n",
    "- Input: [Batch, Time] audio waveforms\n",
    "- Feature extraction: `Wav2Vec2Model`\n",
    "- Averaging over time dimension\n",
    "- Linear Layer → Output score (1–5)\n",
    "\n",
    "##  Evaluation\n",
    "\n",
    "- **Loss**: Mean Squared Error (regression-style)\n",
    "- **Metric**: Accuracy (rounded to nearest integer), Mean Absolute Error (MAE)\n",
    "- The model was trained on 444 samples and evaluated on 195 test samples.\n",
    "- Final predictions are compared with ground truth labels.\n",
    "\n",
    "##  Scoring Rubric Recap\n",
    "\n",
    "| Score | Description |\n",
    "|-------|-------------|\n",
    "| 1 | Frequent basic grammar errors, incomplete sentences |\n",
    "| 2 | Basic grammar and syntax errors throughout |\n",
    "| 3 | Decent grammar but syntax or structure errors |\n",
    "| 4 | Strong grammar with occasional minor mistakes |\n",
    "| 5 | Excellent grammar and control over complex structures |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6871f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load Processor and Pretrained Wav2Vec2 Model ---\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13729820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This is the correct path to your CSV\n",
    "csv_path = r\"C:\\Users\\SIDDHARTH JAIN\\OneDrive\\Desktop\\SHL Project\\train.csv\"\n",
    "\n",
    "#  This is the correct path to your folder containing audio files\n",
    "audio_dir = r\"C:\\Users\\SIDDHARTH JAIN\\OneDrive\\Desktop\\SHL Project\\train_audio\"\n",
    "\n",
    "# Now load the CSV correctly\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature extraction from raw audio\n",
    "def save_wav2vec_features(df, audio_dir, feat_path=\"features.npy\", label_path=\"labels.npy\"):\n",
    "    features, labels = [], []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        path = os.path.join(audio_dir, row['filename'])\n",
    "        label = row['score']\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            out = wav2vec_model(**inputs.to(device)).last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        features.append(out)\n",
    "        labels.append(label)\n",
    "    np.save(feat_path, features)\n",
    "    np.save(label_path, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f79ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once to extract and save features\n",
    "save_wav2vec_features(df, audio_dir, \"features.npy\", \"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b824611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Dataset ---\n",
    "class CachedAudioDataset(Dataset):\n",
    "    def __init__(self, feature_path, label_path=None, file_list_path=None):\n",
    "        self.features = np.load(feature_path)\n",
    "        self.labels = np.load(label_path) if label_path else None\n",
    "        self.file_list = None\n",
    "\n",
    "        if file_list_path:\n",
    "            with open(file_list_path, 'r') as f:\n",
    "                self.file_list = [line.strip() for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        else:\n",
    "            y = torch.tensor(0.0)  # dummy label for test data\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Regression Head ---\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim=768):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7145864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Step 4: Training & Evaluation ---\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=1e-4):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 3\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(x_batch).squeeze()\n",
    "                loss = criterion(output, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses, preds, targets = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                pred = model(x_val).squeeze()\n",
    "                loss = criterion(pred, y_val)\n",
    "                val_losses.append(loss.item())\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                targets.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    print(\"Final MAE:\", mean_absolute_error(targets, preds))\n",
    "    print(\"Final RMSE:\", mean_squared_error(targets, preds, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load preprocessed dataset\n",
    "full_dataset = CachedAudioDataset(\"features.npy\", \"labels.npy\")\n",
    "train_idx, val_idx = train_test_split(np.arange(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Subset(full_dataset, train_idx)\n",
    "val_set = Subset(full_dataset, val_idx)\n",
    "\n",
    "# Increase batch size, reduce num_workers for stability (especially on Windows)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "#  Define model and move to device\n",
    "model = RegressionModel().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Train for fewer epochs first to test speed\n",
    "train_model(model, train_loader, val_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed204927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your test file list\n",
    "test_audio_dir = r\"C:\\Users\\SIDDHARTH JAIN\\OneDrive\\Desktop\\SHL Project\\test_audio\"\n",
    "test_files = sorted([f for f in os.listdir(test_audio_dir) if f.endswith(\".wav\")])\n",
    "\n",
    "# Extract features from test audio\n",
    "test_features = []\n",
    "\n",
    "for fname in tqdm(test_files):\n",
    "    path = os.path.join(test_audio_dir, fname)\n",
    "    audio, sr = librosa.load(path, sr=16000)\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        out = wav2vec_model(**inputs.to(device)).last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    test_features.append(out)\n",
    "\n",
    "np.save(\"features_test.npy\", test_features)\n",
    "print(\" Saved features_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e785c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test features\n",
    "test_features = np.load(\"features_test.npy\")\n",
    "test_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "# Prepare DataLoader\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load your trained model\n",
    "model = RegressionModel()\n",
    "#model.load_state_dict(torch.load(\"best_model.pth\"))  # make sure you saved this after training\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "predictions = []\n",
    "filenames = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        x = test_dataset[i][0]\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        output = model(x).squeeze().item()\n",
    "        predictions.append(output)\n",
    "\n",
    "        if test_files:\n",
    "            filenames.append(test_files)\n",
    "        else:\n",
    "            filenames.append(f\"sample_{i}.wav\")\n",
    "\n",
    "# Save as CSV\n",
    "results_df = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"predicted_score\": predictions\n",
    "})\n",
    "results_df.to_csv(\"predicted_scores.csv\", index=False)\n",
    "print(\" Saved predicted_scores.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9d4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
